# -*- coding: utf-8 -*-
"""Weather_Air_Quality_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d6YG8nthW9ppdK_xd79YwQ3Ln1CgvYkr

# **Data Science Practicum: Weather and Air Quality Analysis**

### Project Overview:

This project analyzes the relationship between meteorological conditions and air quality in major U.S. cities. The analysis begins with a classification model to determine if current weather can predict the current air quality category. After discovering key data challenges, the project pivots to a more advanced time series forecasting model to predict the next day's PM2.5 levels.

## Importing Libraries
"""

import requests
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import folium
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

"""## **Data Collection**

### Data Collection: Weather Data
"""

# Weather Data
weather_url = "https://archive-api.open-meteo.com/v1/archive"

cities = {
    "New York": (40.7128, -74.0060, "36"),   # New York
    "Los Angeles": (34.0522, -118.2437, "06"), # California
    "Houston": (29.7604, -95.3698, "48"),    # Texas
    "Phoenix": (33.4484, -112.0740, "04"),   # Arizona
    "Denver": (39.7392, -104.9903, "08")     # Colorado
}

base_params = {
    'start_date': '2023-01-01',
    'end_date': '2025-10-05',   # Match AQI year
    'daily': 'temperature_2m_max,temperature_2m_min,precipitation_sum,wind_speed_10m_max,relative_humidity_2m_mean,surface_pressure_mean', # Added humidity and pressure
    'timezone': 'America/New_York'
}

weather_data = []

for city, (lat, lon, state_code) in cities.items():
    params = base_params.copy()
    params.update({'latitude': lat, 'longitude': lon})
    response = requests.get(weather_url, params=params).json()

    if "daily" in response:
        df = pd.DataFrame(response["daily"])
        df["date"] = pd.to_datetime(df["time"])
        df["city"] = city
        df["state"] = state_code
        weather_data.append(df)
    else:
        print(f"No weather data for {city}")

weather_df = pd.concat(weather_data, ignore_index=True)

# Update the selection to include the new columns
weather_df = weather_df[[
    "date", "city", "state", "temperature_2m_max", "temperature_2m_min",
    "precipitation_sum", "wind_speed_10m_max", "relative_humidity_2m_mean", "surface_pressure_mean"
]]

print("Weather DataFrame with all features:")
print(weather_df.head())

"""### Data Collection: Air Quality Data (PM2.5)"""

# Air Quality Data by County
aqs_url = "https://aqs.epa.gov/data/api/dailyData/byCounty"
email = "rameswaramkumarviswanath@gmail.com"
key = "amberkit41"

cities = {
    "New York": ("36", "061"),
    "Los Angeles": ("06", "037"),
    "Houston": ("48", "201"),
    "Phoenix": ("04", "013"),
    "Denver": ("08", "031")
}

aqs_data = []
years = ["2023", "2024", "2025"] # List of years to fetch

for city, (state, county) in cities.items():
    for year in years:
        params = {
            "email": email,
            "key": key,
            "param": "88101",  # PM2.5
            "bdate": f"{year}0101",
            "edate": f"{year}1231",
            "state": state,
            "county": county
        }
        r = requests.get(aqs_url, params=params).json()
        if "Data" in r and len(r["Data"]) > 0:
            df = pd.DataFrame(r["Data"])
            df["date"] = pd.to_datetime(df["date_local"])
            df["city"] = city
            df = df[["date", "city", "arithmetic_mean"]]
            aqs_data.append(df)
        else:
            print(f"No AQI data for {city} in {year}")

all_aqs_df = pd.concat(aqs_data, ignore_index=True)

# Aggregate AQI data by date and city
aggregated_aqs_df = all_aqs_df.groupby(['date', 'city'])['arithmetic_mean'].mean().reset_index()
print("\nAggregated AQI DataFrame:")
print(aggregated_aqs_df)

"""## **Data Preparation**

### Merging Datasets
"""

# Merging datasets
final_df = pd.merge(
    weather_df,
    aggregated_aqs_df,
    on=["date","city"],
    how="inner"
)

final_df.rename(columns={"arithmetic_mean":"pm25"}, inplace=True)
print(final_df)

"""## **Exploratory Data Analysis (EDA)**"""

print("\nMissing values in each column:")
print(final_df.isnull().sum())

"""## **Correlation Analysis**"""

# Only numeric columns for correlation
numeric_df = final_df.select_dtypes(include=['number'])
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Between Weather Features and PM2.5')
plt.show()

"""## **Classification Modeling**

### Feature Engineering
"""

def get_aqi_category(pm25):
    if 0 <= pm25 <= 12.0:
        return 'Good'
    elif 12.1 <= pm25 <= 35.4:
        return 'Moderate'
    elif 35.5 <= pm25 <= 55.4:
        return 'Unhealthy for Sensitive Groups'
    elif 55.5 <= pm25 <= 150.4:
        return 'Unhealthy'
    else:
        return 'Very Unhealthy'

final_df['aqi_category'] = final_df['pm25'].apply(get_aqi_category)
print("\nDataFrame with AQI Category:")
print(final_df[['city', 'pm25', 'aqi_category']].head())

"""### Addressing Imbalance: Re-framing as a Binary Problem"""

# Create a new binary target column
# We'll group 'Moderate' and worse into an 'Unacceptable' category
final_df['binary_aqi'] = final_df['aqi_category'].apply(lambda x: 'Good' if x == 'Good' else 'Not Good')

# Check the new distribution - it will be much more balanced!
print(final_df['binary_aqi'].value_counts())

# Display the distribution of AQI categories
print("Air Quality Category Distribution:")
print(final_df['aqi_category'].value_counts())

# Optional: You can also create a quick bar plot
final_df['aqi_category'].value_counts().plot(kind='bar')
plt.title('Distribution of AQI Categories')
plt.ylabel('Number of Days')
plt.show()

"""## **Model Training and Evaluation (Binary Classification)**"""

# Define features (X) and target (y)
features = ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum',
            'wind_speed_10m_max', 'relative_humidity_2m_mean', 'surface_pressure_mean']
target = 'binary_aqi'

# One-hot encode the 'city' column and add it to features
X = pd.get_dummies(final_df[['city'] + features], drop_first=True)
y = final_df[target]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

print("\n--- Random Forest Classifier Results ---")
print(f"Accuracy: {accuracy_score(y_test, rf_predictions):.4f}")
print(classification_report(y_test, rf_predictions))

# Logistic Regression (Baseline)
lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

print("\n--- Logistic Regression Results ---")
print(f"Accuracy: {accuracy_score(y_test, lr_predictions):.4f}")
print(classification_report(y_test, lr_predictions))

# Using the predictions from your Random Forest model
cm = confusion_matrix(y_test, rf_predictions, labels=rf_model.classes_)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Random Forest Model')
plt.xticks(rotation=45)
plt.show()

# Extract feature importances from the trained Random Forest model
importances = rf_model.feature_importances_
feature_names = X_train.columns

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance_df)
plt.title('Feature Importance for Predicting Air Quality')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

# City coordinates from your data collection script
city_coords = {
    "New York": (40.7128, -74.0060),
    "Los Angeles": (34.0522, -118.2437),
    "Houston": (29.7604, -95.3698),
    "Phoenix": (33.4484, -112.0740),
    "Denver": (39.7392, -104.9903)
}

# Calculate average pm25 per city
city_avg_pm25 = final_df.groupby('city')['pm25'].mean()

# Create a map centered on the US
m = folium.Map(location=[39.8283, -98.5795], zoom_start=4)

# Add markers for each city
for city, avg_pm25 in city_avg_pm25.items():
    lat, lon = city_coords[city]
    folium.CircleMarker(
        location=[lat, lon],
        radius=avg_pm25 / 2, # Radius can be proportional to pollution
        popup=f'{city}<br>Avg PM2.5: {avg_pm25:.2f}',
        color='crimson',
        fill=True,
        fill_color='crimson'
    ).add_to(m)
# Display the map
m

"""## **Time Series Forecasting**

### Feature Engineering for Forecasting
"""

# Selected data for one city and it's sorted by date
denver_df = final_df[final_df['city'] == 'Denver'].copy()
denver_df = denver_df.sort_values('date')
denver_df.set_index('date', inplace=True)

# Added Time-Based Features
denver_df['day_of_week'] = denver_df.index.dayofweek
denver_df['month'] = denver_df.index.month
denver_df['day_of_year'] = denver_df.index.dayofyear

# Create lag features
for i in range(1, 4):
    denver_df[f'pm25_lag_{i}'] = denver_df['pm25'].shift(i)
    denver_df[f'temp_max_lag_{i}'] = denver_df['temperature_2m_max'].shift(i)
    denver_df[f'humidity_lag_{i}'] = denver_df['relative_humidity_2m_mean'].shift(i)
    denver_df[f'wind_speed_lag_{i}'] = denver_df['wind_speed_10m_max'].shift(i)

# Added Rolling Window Features
denver_df['pm25_rolling_mean_7'] = denver_df['pm25'].shift(1).rolling(window=7).mean()

# Drop rows with NaN values created by shift() and rolling()
denver_df.dropna(inplace=True)
print("\nDataFrame head with all new features:")
print(denver_df.head())

# The target is the current day's pm25 value
y = denver_df['pm25']

# The features are all the lag columns you created
X = denver_df.drop(columns=['pm25', 'city', 'state', 'temperature_2m_max', 'temperature_2m_min', 'precipitation_sum', 'wind_speed_10m_max', 'relative_humidity_2m_mean', 'surface_pressure_mean', 'aqi_category', 'binary_aqi'])

"""## **Forecasting Model Training and Evaluation**"""

# Split the data into a training and testing set based on time
split_date = '2024-06-01'
X_train = X[X.index < split_date]
X_test = X[X.index >= split_date]

y_train = y[y.index < split_date]
y_test = y[y.index >= split_date]

# Initialize and train the model
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

# Make predictions
predictions = rf_regressor.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, predictions)
print(f'Mean Absolute Error on Test Set: {mae:.2f}')

"""### Visualizing Forecast Performance"""

plt.figure(figsize=(15, 6))
plt.plot(y_test.index, y_test, label='Actual PM2.5')
plt.plot(y_test.index, predictions, label='Predicted PM2.5', linestyle='--')
plt.title('PM2.5 Forecast vs Actuals for Denver')
plt.ylabel('PM2.5 Value')
plt.legend()
plt.show()

"""### Forecasting Model Analysis: Feature Importance

"""

# Add this in a new cell after your model is trained
importances = rf_regressor.feature_importances_
feature_names = X_train.columns

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance_df)
plt.title('Feature Importance for PM2.5 Forecast')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

